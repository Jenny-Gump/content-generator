"""
Batch Processor for Content Generator
–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–º —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ—Ç–æ–∫–∞ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º –∑–∞–≤–∏—Å–∞–Ω–∏–π
"""

import asyncio
import json
import os
import sys
import time
import gc
import signal
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, asdict
import psutil
import requests

from src.logger_config import logger
from batch_config import (
    BATCH_CONFIG, CONTENT_TYPES, BATCH_PATHS, MEMORY_CLEANUP,
    get_content_type_config, get_progress_file_path, get_lock_file_path,
    validate_content_type, ensure_prompts_folder_exists
)


@dataclass
class TopicStatus:
    """–°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Ç–µ–º—ã"""
    topic: str
    status: str  # 'pending', 'processing', 'completed', 'failed'
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    attempts: int = 0
    error_message: Optional[str] = None
    wordpress_id: Optional[int] = None
    wordpress_url: Optional[str] = None


@dataclass
class BatchProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞"""
    content_type: str
    topics_file: str
    total_topics: int
    completed_topics: List[str]
    failed_topics: List[str]
    current_topic: Optional[str]
    start_time: str
    last_update_time: str
    topic_statuses: Dict[str, TopicStatus]


class BatchProcessorError(Exception):
    """–ë–∞–∑–æ–≤–∞—è –æ—à–∏–±–∫–∞ –±–∞—Ç—á-–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
    pass


class TopicProcessingError(BatchProcessorError):
    """–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–º—ã"""
    pass


class PublicationError(BatchProcessorError):
    """–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ WordPress"""
    pass


class MemoryLimitError(BatchProcessorError):
    """–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏"""
    pass


class BatchProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–∏—Å–∫–∞ —Ç–µ–º"""
    
    def __init__(self, topics_file: str, content_type: str = "prompt_collection", 
                 model_overrides: Dict = None, resume: bool = False, 
                 skip_publication: bool = False):
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if not validate_content_type(content_type):
            raise ValueError(f"Unknown content type: {content_type}")
        
        if not os.path.exists(topics_file):
            raise FileNotFoundError(f"Topics file not found: {topics_file}")
        
        self.topics_file = topics_file
        self.content_type = content_type
        self.model_overrides = model_overrides or {}
        self.resume = resume
        self.skip_publication = skip_publication
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.config = get_content_type_config(content_type)
        self.progress_file = get_progress_file_path(content_type)
        self.lock_file = get_lock_file_path(content_type)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.progress: Optional[BatchProgress] = None
        self.is_running = False
        self.shutdown_requested = False
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
        self.process = psutil.Process()
        self.last_memory_check = time.time()
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –ø–∞–ø–∫–∞ —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        ensure_prompts_folder_exists(content_type)
        
        logger.info(f"BatchProcessor initialized for content type: {content_type}")
    
    def _signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown"""
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.shutdown_requested = True
    
    async def process_batch(self) -> bool:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–µ–º—ã –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –≤—Å–µ —Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if self._is_locked() and not self.resume:
                logger.error(f"Batch processing already running for {self.content_type}. Use --resume to continue.")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            self._create_lock()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if self.resume and os.path.exists(self.progress_file):
                self._load_progress()
                logger.info("Resuming previous batch processing session...")
            else:
                self._initialize_progress()
                logger.info("Starting new batch processing session...")
            
            self.is_running = True
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            pending_topics = self._get_pending_topics()
            
            if not pending_topics:
                logger.info("No pending topics found. Batch processing complete!")
                return True
            
            logger.info(f"Processing {len(pending_topics)} pending topics...")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–º—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for topic in pending_topics:
                if self.shutdown_requested:
                    logger.info("Shutdown requested, stopping batch processing...")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–µ–º—ã
                self._check_memory_usage()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–¥–Ω—É —Ç–µ–º—É
                success = await self._process_single_topic(topic)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                self._update_progress(topic, success)
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
                self._cleanup_memory_between_topics()
                
                # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                self._save_progress()
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                if not self.shutdown_requested:
                    await asyncio.sleep(5)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self._log_final_statistics()
            
            return len(self.progress.failed_topics) == 0
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            return False
        finally:
            self._cleanup()
    
    async def _process_single_topic(self, topic: str) -> bool:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Ç–µ–º—É —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–æ—Ç–æ–∫–∞
        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–¢–†–û–ì–û 1 —Ç–µ–º–∞ –∑–∞ —Ä–∞–∑ —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –¥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        """
        logger.info(f"üìù Starting topic: '{topic}'")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        topic_status = TopicStatus(
            topic=topic,
            status='processing',
            start_time=datetime.now().isoformat(),
            attempts=self.progress.topic_statuses.get(topic, TopicStatus(topic, 'pending')).attempts + 1
        )
        self.progress.topic_statuses[topic] = topic_status
        self.progress.current_topic = topic
        
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º main_flow –∑–¥–µ—Å—å —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
            from main import main_flow
            
            # 1. –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
            logger.info(f"üöÄ Executing main pipeline for: {topic}")
            
            # Timeout –¥–ª—è –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–º—ã
            pipeline_task = asyncio.create_task(
                main_flow(
                    topic=topic,
                    model_overrides=self.model_overrides,
                    publish_to_wordpress=not self.skip_publication
                )
            )
            
            try:
                await asyncio.wait_for(pipeline_task, timeout=BATCH_CONFIG["max_topic_timeout"])
            except asyncio.TimeoutError:
                logger.error(f"‚è∞ Topic '{topic}' timed out after {BATCH_CONFIG['max_topic_timeout']} seconds")
                raise TopicProcessingError(f"Processing timeout for topic: {topic}")
            
            # 2. –ë–õ–û–ö–ò–†–û–í–ö–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é –≤ WordPress (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
            if not self.skip_publication and BATCH_CONFIG["verify_publication_before_next"]:
                logger.info(f"üîç Verifying WordPress publication for: {topic}")
                
                publication_verified = await self._verify_wordpress_publication(topic)
                
                if not publication_verified:
                    raise PublicationError(f"WordPress publication not verified for topic: {topic}")
                
                logger.info(f"‚úÖ WordPress publication verified for: {topic}")
            
            # 3. –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            topic_status.status = 'completed'
            topic_status.end_time = datetime.now().isoformat()
            
            logger.info(f"üéâ Topic completed successfully: '{topic}'")
            return True
            
        except Exception as e:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏
            topic_status.status = 'failed'
            topic_status.end_time = datetime.now().isoformat()
            topic_status.error_message = str(e)
            
            logger.error(f"‚ùå Topic failed: '{topic}' - {e}")
            
            # Retry –ª–æ–≥–∏–∫–∞
            max_retries = BATCH_CONFIG["retry_failed_topics"]
            if topic_status.attempts < max_retries:
                logger.info(f"üîÑ Will retry topic '{topic}' (attempt {topic_status.attempts}/{max_retries})")
                topic_status.status = 'pending'  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ pending –¥–ª—è retry
                
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ retry
                await asyncio.sleep(BATCH_CONFIG["retry_delay_seconds"])
                
                return await self._process_single_topic(topic)
            else:
                logger.error(f"üíÄ Topic '{topic}' failed permanently after {max_retries} attempts")
                return False
    
    async def _verify_wordpress_publication(self, topic: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ —Å—Ç–∞—Ç—å—è —Ä–µ–∞–ª—å–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –≤ WordPress
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è –Ω–∞–π–¥–µ–Ω–∞
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ WordPress –∏–∑ .env
            wp_api_url = os.getenv('WORDPRESS_API_URL', 'https://ailynx.ru/wp-json/wp/v2')
            wp_username = os.getenv('WORDPRESS_USERNAME', 'PetrovA')
            wp_password = os.getenv('WORDPRESS_APP_PASSWORD', '')
            
            if not wp_password:
                logger.warning("WordPress credentials not found, skipping publication verification")
                return True
            
            # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
            search_url = f"{wp_api_url}/posts"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
            expected_title_start = topic.strip()
            
            params = {
                'search': expected_title_start,
                'status': 'draft,publish',
                'per_page': 10,
                '_fields': 'id,title,link,status'
            }
            
            response = requests.get(
                search_url,
                params=params,
                auth=(wp_username, wp_password),
                timeout=BATCH_CONFIG["wordpress_api_timeout"]
            )
            
            if response.status_code != 200:
                logger.error(f"WordPress API error: {response.status_code}")
                return False
            
            posts = response.json()
            
            # –ò—â–µ–º –ø–æ—Å—Ç —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            for post in posts:
                post_title = post.get('title', {}).get('rendered', '')
                if expected_title_start.lower() in post_title.lower():
                    logger.info(f"‚úÖ Found published article: {post_title} (ID: {post['id']})")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    topic_status = self.progress.topic_statuses[topic]
                    topic_status.wordpress_id = post['id']
                    topic_status.wordpress_url = post.get('link')
                    
                    return True
            
            logger.warning(f"‚ö†Ô∏è  Article not found in WordPress for topic: {topic}")
            return False
            
        except Exception as e:
            logger.error(f"Error verifying WordPress publication: {e}")
            return False
    
    def _check_memory_usage(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≤—ã–¥–∞–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è"""
        if not BATCH_CONFIG["enable_memory_monitoring"]:
            return
        
        current_time = time.time()
        if current_time - self.last_memory_check < BATCH_CONFIG["memory_check_interval"]:
            return
        
        self.last_memory_check = current_time
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        
        logger.info(f"üß† Memory usage: {memory_mb:.1f}MB")
        
        if memory_mb > BATCH_CONFIG["max_memory_mb"]:
            logger.warning(f"‚ö†Ô∏è  High memory usage: {memory_mb:.1f}MB (limit: {BATCH_CONFIG['max_memory_mb']}MB)")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            self._cleanup_memory_between_topics()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞
            memory_info_after = self.process.memory_info()
            memory_mb_after = memory_info_after.rss / 1024 / 1024
            
            if memory_mb_after > BATCH_CONFIG["max_memory_mb"] * 1.2:  # 20% –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                raise MemoryLimitError(f"Memory usage too high: {memory_mb_after:.1f}MB")
    
    def _cleanup_memory_between_topics(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏"""
        logger.info("üßπ Cleaning up memory between topics...")
        
        if MEMORY_CLEANUP["force_gc_between_topics"]:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π garbage collection
            collected = gc.collect()
            logger.info(f"Garbage collection: {collected} objects collected")
        
        if MEMORY_CLEANUP["clear_llm_caches"]:
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π LLM –∫–ª–∏–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            # TODO: –î–æ–±–∞–≤–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∫—ç—à–µ–π –∫–æ–≥–¥–∞ –æ–Ω–∏ –ø–æ—è–≤—è—Ç—Å—è
            pass
        
        if MEMORY_CLEANUP["reset_token_tracker"]:
            # –°–±—Ä–æ—Å token tracker –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
            pass
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—á–∏—Å—Ç–∫–∏
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        logger.info(f"Memory after cleanup: {memory_mb:.1f}MB")
    
    def _load_topics(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–º –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(self.topics_file, 'r', encoding='utf-8') as f:
                topics = []
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                        topics.append(line)
                return topics
        except Exception as e:
            raise BatchProcessorError(f"Failed to load topics from {self.topics_file}: {e}")
    
    def _get_pending_topics(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–º –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å"""
        all_topics = self._load_topics()
        
        if not self.progress:
            return all_topics
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        completed = set(self.progress.completed_topics)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—ã –¥–ª—è retry (failed —Ç–µ–º—ã —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–ø—ã—Ç–æ–∫ –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞)
        retry_topics = []
        for topic in all_topics:
            if topic in completed:
                continue
                
            topic_status = self.progress.topic_statuses.get(topic)
            if topic_status and topic_status.status == 'failed':
                if topic_status.attempts < BATCH_CONFIG["retry_failed_topics"]:
                    retry_topics.append(topic)
                continue
            
            # –≠—Ç–æ –Ω–æ–≤–∞—è –∏–ª–∏ pending —Ç–µ–º–∞
            if topic not in completed:
                retry_topics.append(topic)
        
        return retry_topics
    
    def _initialize_progress(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ç—á–∞"""
        topics = self._load_topics()
        
        self.progress = BatchProgress(
            content_type=self.content_type,
            topics_file=self.topics_file,
            total_topics=len(topics),
            completed_topics=[],
            failed_topics=[],
            current_topic=None,
            start_time=datetime.now().isoformat(),
            last_update_time=datetime.now().isoformat(),
            topic_statuses={topic: TopicStatus(topic, 'pending') for topic in topics}
        )
        
        logger.info(f"Initialized batch processing for {len(topics)} topics")
    
    def _load_progress(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–±—ä–µ–∫—Ç—ã
            topic_statuses = {}
            for topic, status_data in data['topic_statuses'].items():
                topic_statuses[topic] = TopicStatus(**status_data)
            
            self.progress = BatchProgress(
                content_type=data['content_type'],
                topics_file=data['topics_file'],
                total_topics=data['total_topics'],
                completed_topics=data['completed_topics'],
                failed_topics=data['failed_topics'],
                current_topic=data['current_topic'],
                start_time=data['start_time'],
                last_update_time=data['last_update_time'],
                topic_statuses=topic_statuses
            )
            
            logger.info(f"Loaded progress: {len(self.progress.completed_topics)}/{self.progress.total_topics} completed")
            
        except Exception as e:
            logger.error(f"Failed to load progress: {e}")
            self._initialize_progress()
    
    def _save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ñ–∞–π–ª"""
        if not self.progress:
            return
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON
            progress_dict = asdict(self.progress)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º TopicStatus –æ–±—ä–µ–∫—Ç—ã
            progress_dict['topic_statuses'] = {
                topic: asdict(status) for topic, status in self.progress.topic_statuses.items()
            }
            
            progress_dict['last_update_time'] = datetime.now().isoformat()
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_dict, f, indent=2, ensure_ascii=False)
                
            logger.debug("Progress saved successfully")
            
        except Exception as e:
            logger.error(f"Failed to save progress: {e}")
    
    def _update_progress(self, topic: str, success: bool):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–º—ã"""
        if not self.progress:
            return
        
        if success:
            if topic not in self.progress.completed_topics:
                self.progress.completed_topics.append(topic)
            
            # –£–±–∏—Ä–∞–µ–º –∏–∑ failed –µ—Å–ª–∏ –±—ã–ª–∞ —Ç–∞–º
            if topic in self.progress.failed_topics:
                self.progress.failed_topics.remove(topic)
        else:
            if topic not in self.progress.failed_topics:
                self.progress.failed_topics.append(topic)
        
        self.progress.current_topic = None
        self.progress.last_update_time = datetime.now().isoformat()
    
    def _log_final_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        if not self.progress:
            return
        
        total = self.progress.total_topics
        completed = len(self.progress.completed_topics)
        failed = len(self.progress.failed_topics)
        
        logger.info("üìä Batch Processing Statistics:")
        logger.info(f"   Total topics: {total}")
        logger.info(f"   Completed: {completed} ({completed/total*100:.1f}%)")
        logger.info(f"   Failed: {failed} ({failed/total*100:.1f}%)")
        
        if failed > 0:
            logger.info("‚ùå Failed topics:")
            for topic in self.progress.failed_topics:
                status = self.progress.topic_statuses.get(topic)
                if status and status.error_message:
                    logger.info(f"   - {topic}: {status.error_message}")
                else:
                    logger.info(f"   - {topic}")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        start_time = datetime.fromisoformat(self.progress.start_time)
        total_duration = datetime.now() - start_time
        logger.info(f"‚è±Ô∏è  Total duration: {total_duration}")
        
        if completed > 0:
            avg_time_per_topic = total_duration / completed
            logger.info(f"üìà Average time per topic: {avg_time_per_topic}")
    
    def _is_locked(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞"""
        if not os.path.exists(self.lock_file):
            return False
        
        try:
            with open(self.lock_file, 'r') as f:
                pid = int(f.read().strip())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å
            return psutil.pid_exists(pid)
        except:
            return False
    
    def _create_lock(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        with open(self.lock_file, 'w') as f:
            f.write(str(os.getpid()))
    
    def _remove_lock(self):
        """–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        if os.path.exists(self.lock_file):
            os.remove(self.lock_file)
    
    def _cleanup(self):
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.info("üßπ Cleaning up batch processor...")
        
        self.is_running = False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        if self.progress:
            self._save_progress()
        
        # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        self._remove_lock()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        self._cleanup_memory_between_topics()
        
        logger.info("Batch processor cleanup completed")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
async def run_batch_processor(topics_file: str, content_type: str = "prompt_collection",
                             model_overrides: Dict = None, resume: bool = False,
                             skip_publication: bool = False) -> bool:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç batch processor —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –≤—Å–µ —Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ
    """
    processor = BatchProcessor(
        topics_file=topics_file,
        content_type=content_type,
        model_overrides=model_overrides,
        resume=resume,
        skip_publication=skip_publication
    )
    
    return await processor.process_batch()


# CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Batch processor for Content Generator")
    parser.add_argument("topics_file", help="Path to file with topics (one per line)")
    parser.add_argument("--content-type", default="prompt_collection", 
                       choices=list(CONTENT_TYPES.keys()), help="Content type")
    parser.add_argument("--resume", action="store_true", help="Resume previous batch")
    parser.add_argument("--skip-publication", action="store_true", help="Skip WordPress publication")
    parser.add_argument("--extract-model", help="Override extraction model")
    parser.add_argument("--generate-model", help="Override generation model") 
    parser.add_argument("--editorial-model", help="Override editorial model")
    
    args = parser.parse_args()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ model_overrides
    model_overrides = {}
    if args.extract_model:
        model_overrides["extract_prompts"] = args.extract_model
    if args.generate_model:
        model_overrides["generate_article"] = args.generate_model
    if args.editorial_model:
        model_overrides["editorial_review"] = args.editorial_model
    
    # –ó–∞–ø—É—Å–∫ batch processor
    try:
        success = asyncio.run(run_batch_processor(
            topics_file=args.topics_file,
            content_type=args.content_type,
            model_overrides=model_overrides if model_overrides else None,
            resume=args.resume,
            skip_publication=args.skip_publication
        ))
        
        if success:
            print("‚úÖ Batch processing completed successfully")
            sys.exit(0)
        else:
            print("‚ùå Batch processing completed with errors")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nüõë Batch processing interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"üí• Batch processing failed: {e}")
        sys.exit(1)